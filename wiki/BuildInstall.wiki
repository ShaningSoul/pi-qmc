#summary How to build and install the pi program

= Introduction =


The easiest way to build is to use:
{{{
./configure
make
}}}

For a parallel build
{{{
./configure --enable-mpi MPICXX=mpic++ MPICC=mpicc MPIF77=mpif77
}}}
where you should use the names of your MPI enabled compilers.

You can also build for different numbers of physical dimensions (default is NDIM=3)
{{{
./configure --with-ndim=2
}}}

= Required libraries =
We use the following libraries in the *_pi_* code:
  * [http://xmlsoft.org/ libxml2]
  * [http://www.oonumerics.org/blitz/ blitz++]
  * [http://www.hdfgroup.org/ hdf5]
  * [http://www.fftw.org/ fftw3]
  * BLAS/LAPACK
  * [http://www.gnu.org/software/gsl/ gsl]

= Advanced build using multiple directories=

In research, we often want different versions of the executables, for example, versions with and
without MPI, or versions compiled for two-dimensional systems. To accomplish this, we make 
a <tt>pibuilds</tt> directory beside our svn checkout directory (<tt>pi</tt> or <tt>pi-qmc</tt>).
We then make empty subdirectories for each build, for example <tt>ndim2mpi</tt> for a two
dimensional MPI version. A typical directory structure is:

{{{
codes/
  pi-qmc/
    configure
    src/
    lib/
  pibuilds/
    ndim1/
    ndim2/
    ndim3/
    ndim1mpi/
    ndim2mpi/
    ndim3mpi/
    debug/
}}}

To build, go into the empty build directory,
<code language="sh">
cd ~/codes/pibuilds/ndim2mpi
</code>
Then run the configure script with the desired options
<code language="sh">
../../configure --with-ndim=2 --enable-mpi
</code>
You will probably want more configure options; see the platform specific instructions
below for some examples.

Then, make the code in that directory,
<code language="sh">
make -j2
</code>

For conveniance, you can make a soft link to the executable
<code language="sh">
ln -sf ~/codes/pibuilds/ndim3mpi ~/bin/pi2Dmpi
</code>

= Platform specific instructions =

==Mac OS X==

All the dependencies are available through [http://www.macports.org/ macports]. It is also handy to install
the latest gcc compilers (with gfortran), openmpi, and python utilities for data analysis and plotting.
{{{
$ port installed
  libxml2 @2.7.3_0 (active)
  blitz @0.9_0 (active)
  hdf5-18 @1.8.3_0 (active)
  gsl @1.12_0 (active)

  gcc44 @4.4.0_0 (active)

  python26 @2.6.2_3 (active)
  py26-numpy @1.3.0_0 (active)
  py26-ipython @0.9.1_0+scientific (active)
  py26-scipy @0.7.0_0+gcc44 (active)
  py26-tables @2.1_0 (active)
}}}

The following configure works well on an intel mac:
<code language="sh">
../../pi/configure CXX=g++-mp-4.4 CC=gcc-mp-4.4 \
CXXFLAGS="-O3 -g -Wall -ffast-math -ftree-vectorize \
-march=prescott -fomit-frame-pointer -pipe" \ 
F77=gfortran-mp-4.4
</code>
or, for an MPI enabled build,
<code language="sh">
../../pi/configure --enable-mpi CXX=g++-mp-4.4 CC=gcc-mp-4.4 F77=gfortran-mp-4.4 \
MPICC=openmpicc MPICXX=openmpicxx MPIF77=openmpif77 \
CXXFLAGS="-O3 -g -Wall -ffast-math -ftree-vectorize \
-march=prescott -fomit-frame-pointer -pipe"
</code>

On a G5 mac, try:
<code language="sh">
../../pi/configure --with-ndim=3  F77=gfortran-mp-4.4 CC=gcc-mp-4.4 CXX=g++-mp-4.4\
CXXFLAGS="-g -O3 -ffast-math -ftree-vectorize -maltivec -mpowerpc-gpopt \
-mpowerpc64 falign-functions=32 -falign-labels=32 -falign-loops=32 -falign-jumps=32 -funroll-loops"
</code>
or, for an MPI enabled build,
<code language="sh">
../../pi/configure --with-ndim=3 --enable-mpi \
CXXFLAGS="-g -O3 -ffast-math -ftree-vectorize -maltivec -mpowerpc-gpopt \
-mpowerpc64 falign-functions=32 -falign-labels=32 -falign-loops=32 -falign-jumps=32 -funroll-loops" \
 F77=gfortran-mp-4.4 CC=gcc-mp-4.4 CXX=g++-mp-4.4  MPICC=openmpicc MPICXX=openmpicxx MPIF77=openmpif77
</code>




==Linux==

==HPC Centers==

===ASU Fulton: saguaro===
For a serial build in two dimensions,
<code languge="sh">
../../pi/configure --with-ndim=2 --enable-sprng  CXX=icpc CC=icc CXXFLAGS="-O3 -xP -ipo" \
--with-blas="-L$MKL_LIB -lmkl_lapack -lmkl_intel_lp64 -lmkl_sequential -lmkl_core" \
F77=ifort AR="xild -lib"
</code>
or for a parallel version,
<code languge="sh">
../../pi/configure --with-ndim=2 --enable-sprng --enable-mpi MPICC=mpicc MPICXX=mpicxx \
CXX=icpc CC=icc F77=ifort CXXFLAGS="-O3 -xP -ipo" AR="xild -lib" \
--with-blas="-L$MKL_LIB -lmkl_lapack -lmkl_intel_lp64 -lmkl_sequential -lmkl_core" 
</code>
Omit the <tt>--enable-sprng</tt> option if you do not have the SPRNG library.

===LONI-LSU: queenbee ===
You need to add some lines to your <tt>.soft</tt> file to include some required libraries,
<code language="sh">
#My additions (CPATH mimics -I include directories).
CPATH += /usr/local/packages/hdf5-1.8.1-intel10.1/include
+gsl-1.9-intel10.1
+sprng4-mvapich-1.1-intel-10.1
+fftw-3.1.2-intel10.1
CPATH += :/usr/local/packages/fftw-3.1.2-intel10.1/include
+intel-mkl
CPPFLAGS += -DMPICH_IGNORE_CXX_SEEK
</code>
For an MPI build, use,
<code langauge="sh">
../../pi/configure --with-ndim=3 --enable-mpi MPICC=mpicc MPICXX=mpicxx \
CXX=icpc CC=icc F77=ifort AR="xild -lib" CXXFLAGS="-O3 -xP -ipo" \
--with-blas="-lmkl_lapack -lmkl_intel_lp64 -lmkl_sequential -lmkl_core"
</code>
===NCSA: abe===

You need to add some lines to your <tt>.soft</tt> file to include some required libraries,
<code language="sh">
#My additions (CPATH mimics -I include directories).
+intel-mkl
+gsl-intel
+hdf5-1.8.2
CPATH += :/usr/apps/hdf/hdf5/v182/include
LD_LIBRARY_PATH += /usr/apps/hdf/szip/lib
+fftw-3.1-intel
LD_LIBRARY_PATH += /usr/apps/math/fftw/fftw-3.1.2/intel10/lib
CPATH += :/usr/apps/math/fftw/fftw-3.1.2/intel10/include
+intel-mkl
CPPFLAGS = "${CPPFLAGS} -DMPICH_IGNORE_CXX_SEEK"
</code>
For an MPI build, use,
<code language="sh">
../../pi/configure --with-ndim=3 --enable-mpi MPICC=mpicc MPICXX=mpicxx CXX=icpc CC=icc \
CXXFLAGS="-O3 -xP -ipo" LDFLAGS="-lsz" \
--with-blas="-lmkl_lapack -lmkl_intel_lp64 -lmkl_sequential -lmkl_core" F77=ifort AR="xild -lib"
</code>
===TACC: Ranger===

===Cornell CNF: nanolab===
The svn client wasn't working for me, so I built one in my ~/packages/bin directory. You need to specify the most recent C++ and Fortran compilers by including the following in your .bash_profile,
<code language="sh">
# Version 10 compilers
source /opt/intel/cc/10.1.017/bin/iccvars.sh
source /opt/intel/fc/10.1.017/bin/ifortvars.shsource /opt/intel/idb/10.1.017/bin/idbvars.sh
source /opt/intel/mkl/10.0.4.023/tools/environment/mklvars32.sh
</code>
Also, make sure that <tt>/usr/lam-7.4.1_intelv10/bin</tt> is in your path to get the correct MPI compilers.

You need to build blitz (again, in my ~/packages directory). For a serial *_pi_* build,
<code language="sh">
../../pi/configure --with-ndim=3 CXX=icpc CC=icc CXXFLAGS="-O3 -ipo" \
--with-blas="-Wl,-rpath,$MKLROOT/lib/32 -L/opt$MKLROOT/lib/32 -lmkl_intel \
-lmkl_sequential -lmkl_core -lpthread -lm" F77=ifort AR="xild -lib"
</code>

<code language="sh">
../../pi/configure --with-ndim=3 CXX=icpc CC=icc CXXFLAGS="-O3 -ipo" \
--with-blas="-Wl,-rpath,$MKLROOT/lib/32 -L$MKLROOT/lib/32 -lmkl_intel \
-lmkl_sequential -lmkl_core -lpthread -lm" F77=ifort AR="xild -lib" \
--enable-mpi MPICXX=mpic++ MPICC=mpicc MPIF77=mpif77
</code>